{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17632c76",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "## Part 1: Download the Wikipedia pages of characters\n",
    "\n",
    "Now, it's time to go and get the names of all the wiki pages you'll need for your analysis. Those will serve as the nodes in our network.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Go to the page https://en.wikipedia.org/wiki/List_of_mainstream_rock_performers and extract all of the artist-links using your regular expressions from above.\n",
    "  - Hint: To make this easier, you can simply hit the edit button on wikipedia, copy the entire content of the file to a plain text file on your computer and manually delete all of the markup that's not related to the artists' names. (Otherwise there are some wiki-links there that you don't want)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wiki_links(text):\n",
    "    import re\n",
    "    \n",
    "    pattern = r'\\[\\[([\\w\\s./\\(\\)]+)(?:\\|[\\w\\s./\\(\\)]+)?\\]\\]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def insert_underscores(list_of_titles):\n",
    "    return [title.replace(' ', '_') for title in list_of_titles]\n",
    "\n",
    "def create_wiki_link(list_of_titles):\n",
    "    wiki_prefix = \"https://en.wikipedia.org/wiki/\"\n",
    "    links = [wiki_prefix + title for title in list_of_titles]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19047b77",
   "metadata": {},
   "source": [
    "Extract all names of the wikipages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a70f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10cc', '10 Years (band)', '3 Doors Down', '311 (band)', '38 Special (band)', 'ABBA', 'Accept (band)', 'AC/DC', 'Bryan Adams', 'Aerosmith']\n",
      "['10cc', '10_Years_(band)', '3_Doors_Down', '311_(band)', '38_Special_(band)', 'ABBA', 'Accept_(band)', 'AC/DC', 'Bryan_Adams', 'Aerosmith']\n"
     ]
    }
   ],
   "source": [
    "# take all text from the .txt file\n",
    "wiki_txt = open('List_of_mainstream_rock_performers.txt', 'r', encoding='utf-8')\n",
    "text = wiki_txt.read()\n",
    "wiki_txt.close()\n",
    "\n",
    "# extract all wiki links from the text\n",
    "list_of_performers = extract_wiki_links(text)\n",
    "bands = insert_underscores(list_of_performers)\n",
    "print(list_of_performers[:10])\n",
    "print(bands[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c106f0a",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644d9ab",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "Use your knowledge of APIs and the list of all the wiki-pages to download all the text on the pages of the country performers.\n",
    "- Hint 0: Make sure you read the Wiki API pages to ensure that your download the cleanest possible version of the page (the wikitext). This link may be helpful.\n",
    "- Hint 1: You may want to save the individual band/artist pages on your computer. You can use your skills from the first lectures to write them as plain-text files (that's what I would do - one file per band/artist, named according to its wiki-link). (But you can also use pickle files or start a database if you like that better.)\n",
    "- Hint 2: If you now have a directory with all those files, you can use os.listdir() to list all the files in that directory within Python and iterate over the files if you need to.\n",
    "- Hint 3: Don't forget to add underscores to the performer names when you construct the urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a wikipedia site name, extract the page text using the wikipedia API\n",
    "def get_wiki_page_text(page_name):\n",
    "    import json\n",
    "    import urllib.request\n",
    "\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={page_name}\"\n",
    "    content = \"prop=extracts&rvprop=explaintext\"\n",
    "    dataformat =\"format=json\"\n",
    "    \n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; ExerciseBot/1.0)'} # Set a user-agent to avoid potential blocking by wikipedia\n",
    "    req_query = urllib.request.Request(query, headers=headers)\n",
    "    wikiresponse = urllib.request.urlopen(req_query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    \n",
    "    json_wikitext = json.loads(wikitext)\n",
    "    \n",
    "    page_id = next(iter(json_wikitext['query']['pages']))\n",
    "    page_text = json_wikitext['query']['pages'][page_id]['extract']\n",
    "    \n",
    "    return page_text\n",
    "\n",
    "# test with fist performer on the list list_of_performers\n",
    "# print(get_wiki_page_text(bands[1]))\n",
    "\n",
    "# save all pages as text files in a directory (AC/DC is missing)\n",
    "import os\n",
    "output_dir = 'performer_pages'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for performer in bands:\n",
    "    try:\n",
    "        page_text = get_wiki_page_text(performer)\n",
    "        with open(os.path.join(output_dir, f\"{performer}.txt\"), 'w', encoding='utf-8') as f:\n",
    "            f.write(page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve page for {performer}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d02f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Funkadelic.txt', 'Slayer.txt', 'Ted_Nugent.txt', 'Great_White.txt', 'Days_of_the_New.txt', 'The_Dave_Clark_Five.txt', 'Keane_(band).txt', 'Jimmy_Eat_World.txt', 'Flogging_Molly.txt', 'Simple_Plan.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path=output_dir)[:10])  # List first 10 files in the directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
