{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b226b7b8",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Description\n",
    "\n",
    "Our analysis is based on the **Spotify Million Playlist Dataset (MPD)**, introduced as part of the RecSys Challenge 2018 by Spotify Research [1]. The dataset contains **1,000,000 user-generated playlists**, sampled from over 4 billion public playlists created on the Spotify platform between January 2010 and November 2017. Each playlist includes metadata such as playlist title, number of tracks, number of albums, and duration, as well as detailed track-level information including track name, artist name, album name, and track duration. In total, the MPD comprises **over 2 million unique tracks by nearly 300,000 artists**. Playlists were sampled with randomization and manually filtered to ensure quality and remove offensive content [2].\n",
    "\n",
    "In our report we have decided to use the first 1000 playlists as the foundation for our network. \n",
    "\n",
    "To enrich the network with semantic information, we scraped lyrics for the tracks associated with each artist from the Genius website[3] using a Genius API[4]. For each artist, we collected the full text of their songs, including verses, choruses, and bridges, and stored them in plain text files. These files were aggregated at the artist level, resulting in a single combined lyric corpus per artist.\n",
    "\n",
    "**References**  \n",
    "[1] Spotify Research. *The Million Playlist Dataset Challenge*. RecSys Challenge 2018. Available at: [https://www.aicrowd.com/challengesspotify-million-playlist-dataset-challenge  \n",
    "[2] McFee, B., et al. (2018). *The Million Playlist Dataset Challenge*. Proceedings of the ACM RecSys Challenge 2018.  \n",
    "[3] https://genius.com/  \n",
    "[4] https://lyricsgenius.readthedocs.io/en/master/reference/genius.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e362f4",
   "metadata": {},
   "source": [
    "## Loading the playlist data into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63df198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 files, total playlists merged: 1000\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import lyricsgenius\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "# number of playlist to process out of the slice files\n",
    "NUM_PLAYLISTS = 1000\n",
    "\n",
    "# Path to your original slice file\n",
    "folder_path = Path(r\"/Users/noa/Desktop/02805 - Social Graphs/playlist_data/\")\n",
    "\n",
    "# folder with artist/lyrics files\n",
    "artist_folder = Path(r\"/Users/noa/Desktop/02805 - Social Graphs/artist_lyrics_cleaned\")\n",
    "\n",
    "# Load all mpd slice JSON files in the folder and merge their playlists\n",
    "file_list = sorted(folder_path.glob(\"mpd.slice.*.json\"))\n",
    "playlists = []\n",
    "for fp in file_list:\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        playlists.extend(data.get(\"playlists\", []))\n",
    "\n",
    "print(f\"Loaded {len(file_list)} files, total playlists merged: {len(playlists[:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caa065",
   "metadata": {},
   "source": [
    "We have decided to filter out some of the playlists using the following criteria:\n",
    "- A playlist should contain between 20 and 100 songs\n",
    "  - This is done such that we don't have playlists without themes, as we expect playlists of length > 100 to be more randomly sampled and less curated.\n",
    "  - Playlists of less than 20 songs might not have enough songs for it to have thematic meaning.\n",
    "- Each playlist should contain at least 6 different artists. \n",
    "  - this is done to ensure variety in the playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb195b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included playlists: 583\n"
     ]
    }
   ],
   "source": [
    "# Accumulators\n",
    "artist_songs = defaultdict(set)\n",
    "artist_playlists = defaultdict(set)\n",
    "artist_albums = defaultdict(set)\n",
    "artist_durations = defaultdict(list)\n",
    "edge_playlists = defaultdict(set)\n",
    "\n",
    "def normalize_artist_name(name):\n",
    "    if not name:\n",
    "        return None\n",
    "    return name.replace(' ', '_').strip()\n",
    "\n",
    "included_playlists = 0\n",
    "\n",
    "for pl in playlists[:NUM_PLAYLISTS]:\n",
    "    pid = pl.get(\"pid\")\n",
    "    tracks = pl.get(\"tracks\", [])\n",
    "    # filter playlists by track count and unique artist count\n",
    "    if not (20 <= len(tracks) <= 100):\n",
    "        continue\n",
    "\n",
    "    # build normalized set of unique artists for this playlist\n",
    "    unique_artists = {normalize_artist_name(t[\"artist_name\"]) for t in tracks if t.get(\"artist_name\")}\n",
    "    unique_artists = {a for a in unique_artists if a}  # drop Nones/empty\n",
    "    if len(unique_artists) < 6:\n",
    "        continue\n",
    "\n",
    "    included_playlists += 1\n",
    "\n",
    "    # collect songs, albums, durations, playlist membership per artist\n",
    "    for t in tracks:\n",
    "        raw_artist = t.get(\"artist_name\")\n",
    "        artist = normalize_artist_name(raw_artist)\n",
    "        if not artist:\n",
    "            continue\n",
    "        track_name = t.get(\"track_name\")\n",
    "        album_name = t.get(\"album_name\")\n",
    "        duration = t.get(\"duration_ms\")\n",
    "\n",
    "        if track_name:\n",
    "            artist_songs[artist].add(track_name)\n",
    "        if album_name:\n",
    "            artist_albums[artist].add(album_name)\n",
    "        if duration:\n",
    "            artist_durations[artist].append(duration)\n",
    "        artist_playlists[artist].add(pid)\n",
    "\n",
    "    # increment edge counters for every pair of (normalized) artists in this playlist\n",
    "    for a, b in combinations(sorted(unique_artists), 2):\n",
    "        edge_playlists[(a, b)].add(pid)\n",
    "\n",
    "print(f\"Included playlists: {included_playlists}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91a76f",
   "metadata": {},
   "source": [
    "The assumptions have filtered out approximately half of the playlists. It is important to remark that the lyrics that is added to all artists as an attribute, is the scraping from all 1000 playlists text from Genius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f757f8",
   "metadata": {},
   "source": [
    "## Scrape the lyrics\n",
    "\n",
    "OBS! This code was run on the HPC and all concatenated lyrics were saved in txt files.\n",
    "\n",
    "We have had these considerations\n",
    "- Avoid duplicates: Only fetch lyrics for unique songs per artist.\n",
    "- Rate limits: Genius API has limits, so we have needed to add delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8df6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import lyricsgenius\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Folder to save lyrics\n",
    "lyrics_folder = Path(\"artist_lyrics\")\n",
    "lyrics_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Setup Genius API\n",
    "GENIUS_ACCESS_TOKEN = \"IKoqZN1ANyU_2G6zmTPF2xlaH2OlIEEUlDoD97Mo9-P_A6-2QgnSoQlwsJ3Hy3DY\"  # <--- paste your token\n",
    "\n",
    "# Initialize Genius client\n",
    "genius = lyricsgenius.Genius(\n",
    "    GENIUS_ACCESS_TOKEN,\n",
    "    remove_section_headers=True,   # cleans up [Verse], etc.\n",
    "    timeout=15,\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "artist_lyrics = defaultdict(str)\n",
    "\n",
    "for artist, songs in artist_songs.items():\n",
    "    for track_name in songs:\n",
    "        try:\n",
    "            song = genius.search_song(track_name, artist)\n",
    "            if song and song.lyrics:\n",
    "                artist_lyrics[artist] += \"\\n\" + song.lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {track_name} by {artist}: {e}\")\n",
    "        time.sleep(1)  # Avoid hitting rate limits\n",
    "\n",
    "# Save lyrics to files\n",
    "for artist, lyrics in artist_lyrics.items():\n",
    "    safe_name = re.sub(r'[^\\w\\s-]', '', artist).strip().replace(' ', '_')\n",
    "    file_path = lyrics_folder / f\"{safe_name}.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd6206",
   "metadata": {},
   "source": [
    "## Clean and load lyrics for all artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Union\n",
    "import argparse\n",
    "\n",
    "def remove_metadata_blocks_from_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove metadata blocks from a text.\n",
    "    Returns the cleaned text.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines(keepends=True)\n",
    "    out_lines = []\n",
    "    i = 0\n",
    "\n",
    "    # match any line that starts with optional spaces, then digits, then 'Contributors'\n",
    "    pattern = re.compile(r'^\\s*\\d+\\s+Contributors')  # matches start of metadata block\n",
    "\n",
    "    #match any contiguous non-space \"word\" that ends with 'Embed'\n",
    "    embed_pat = re.compile(r'\\b\\S+Embed\\b', re.IGNORECASE)\n",
    "\n",
    "    while i < len(lines):\n",
    "        # remove tokens like '87Embed' but NOT standalone 'Embed'\n",
    "        line = embed_pat.sub('', lines[i])\n",
    "\n",
    "        if pattern.match(line):\n",
    "            # skip this metadata line and subsequent non-blank lines\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].strip() != \"\":\n",
    "                i += 1\n",
    "            # if there's a blank line, preserve it (to keep stanza breaks)\n",
    "            if i < len(lines) and lines[i].strip() == \"\":\n",
    "                out_lines.append(lines[i])\n",
    "                i += 1\n",
    "        else:\n",
    "            out_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    return \"\".join(out_lines)\n",
    "\n",
    "\n",
    "def process_file(path: Union[str, Path], inplace: bool = True, backup: bool = True, encoding: str = \"utf-8\") -> str:\n",
    "    \"\"\"\n",
    "    Process a single file. If inplace is True, overwrite the file (optionally making a .bak backup).\n",
    "    Returns the cleaned text.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    text = p.read_text(encoding=encoding)\n",
    "    cleaned = remove_metadata_blocks_from_text(text)\n",
    "\n",
    "    if inplace:\n",
    "        if backup:\n",
    "            bak = p.with_suffix(p.suffix + \".bak\")\n",
    "            bak.write_text(text, encoding=encoding)\n",
    "        p.write_text(cleaned, encoding=encoding)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_paths(paths: Iterable[Union[str, Path]], **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Process multiple files or directories. If a directory is supplied, all files\n",
    "    inside (non-recursive) will be processed. kwargs are passed to process_file.\n",
    "    \"\"\"\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if p.is_dir():\n",
    "            for child in p.iterdir():\n",
    "                if child.is_file():\n",
    "                    process_file(child, **kwargs)\n",
    "        elif p.is_file():\n",
    "            process_file(p, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all files in a folder\n",
    "PATH = '/Users/noa/Desktop/02805 - Social Graphs/artist_lyrics_cleaned' # TODO\n",
    "process_paths([PATH], inplace=True, backup=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce09c81",
   "metadata": {},
   "source": [
    "Now we create the dictionary of the lyrics for each artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fe74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_dict = {}\n",
    "\n",
    "# Load lyrics for artists\n",
    "for txt_file in artist_folder.glob(\"*.txt\"):\n",
    "    artist_name = txt_file.stem  # filename without extension\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        lyrics_dict[artist_name.lower()] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6e808",
   "metadata": {},
   "source": [
    "## Creating the undirected co-occurence graph\n",
    "From the MPD, we construct an **artist co-occurrence network** where nodes represent artists and edges represent co-occurrence in playlists. An edge between two artists indicates that they appear together in at least one playlist. \n",
    "\n",
    "**Nodes** represent artists.\n",
    "Node attributes:\n",
    "- songs: set of track names\n",
    "- playlists: set of playlist IDs\n",
    "- num_playlists: count of playlists\n",
    "- num_songs: count of songs\n",
    "- avg_song_duration: average track duration\n",
    "- albums: set of album names\n",
    "- lyrics: if available (for artists like 2Pac)\n",
    "\n",
    "**Edges** represent co-occurrence in playlists.\n",
    "Edge attributes:\n",
    "- shared_playlists: set of playlist IDs\n",
    "- weight: number of shared playlists\n",
    "- co_occurrence_count: number of times they appear together\n",
    "\n",
    "We are not keeping nodes where:\n",
    "- the Genius API was not able to find the lyrics of the artist\n",
    "- Artist that have less than four songs represented in the 1000 playlists\n",
    "\n",
    "Lastly we only keep the largest connected component as the main graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41eceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing artists with no lyrics: 1015 nodes, 63840 edges\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for artist in artist_songs.keys():\n",
    "    num_playlists = len(artist_playlists[artist])\n",
    "    num_songs = len(artist_songs[artist])\n",
    "    avg_duration = sum(artist_durations[artist]) / len(artist_durations[artist]) if artist_durations[artist] else None\n",
    "    lyrics = lyrics_dict.get(artist.lower(), None)\n",
    "\n",
    "    G.add_node(artist,\n",
    "               songs=list(artist_songs[artist]),\n",
    "               albums=list(artist_albums[artist]),\n",
    "               playlists=list(artist_playlists[artist]),\n",
    "               num_playlists=num_playlists,\n",
    "               num_songs=num_songs,\n",
    "               avg_song_duration=avg_duration,\n",
    "               lyrics=lyrics)\n",
    "\n",
    "# Add edges with attributes\n",
    "for (a, b), pls in edge_playlists.items():\n",
    "    G.add_edge(a, b,\n",
    "               shared_playlists=list(pls),\n",
    "               weight=len(pls),\n",
    "               co_occurrence_count=len(pls))\n",
    "\n",
    "# remove nodes where lyrics attribute is missing or empty\n",
    "nodes_to_remove = [n for n, attrs in G.nodes(data=True) if not attrs.get('lyrics')]\n",
    "\n",
    "# remove nodes from the graph where num_songs is less than 4 \n",
    "nodes_to_remove += [n for n, attrs in G.nodes(data=True) if attrs.get('num_songs', 0) < 4]\n",
    "\n",
    "G.remove_nodes_from(nodes_to_remove) \n",
    "\n",
    "# keep largest connected component only\n",
    "if not nx.is_connected(G):\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "print(f\"After removing artists with no lyrics: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457b33f",
   "metadata": {},
   "source": [
    "## Backbone\n",
    "\n",
    "Because our network is very dense, we are going to filter out some nodes and edges finding the backbone of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3894c2",
   "metadata": {},
   "source": [
    "Skal vi mÃ¥ske bruge denne til at beregne modularitet?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G, partition):\n",
    "    \"\"\"\n",
    "    Compute modularity using Eq. 9.12:\n",
    "    \"\"\"\n",
    "    L = G.number_of_edges()  # total edges\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    M = 0.0\n",
    "    for community in partition:\n",
    "        # Internal edges in community\n",
    "        L_c = G.subgraph(community).number_of_edges()\n",
    "        # Sum of degrees in community\n",
    "        k_c = sum(degrees[node] for node in community)\n",
    "        M += (L_c / L) - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
