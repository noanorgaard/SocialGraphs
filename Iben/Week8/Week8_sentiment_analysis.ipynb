{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98132f6b",
   "metadata": {},
   "source": [
    "## Part A: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a4958",
   "metadata": {},
   "source": [
    "#### Exercise: Sentiment distribution.\n",
    "\n",
    "- Download the LabMT wordlist. It's available as supplementary material from Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter (Data Set S1). Describe briefly how the list was generated.\n",
    "- Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc).\n",
    "- Iterage over the nodes in your network, tokenize each page, and calculate sentiment every single page. Now you have sentiment as a new nodal property.\n",
    "- Calculate the average sentiment across all the pages. Also calculate the median, variance, 25th percentile, 75th percentile.\n",
    "- Remember histograms? Create a histogram of all of the artists's associated page-sentiments. (And make it a nice histogram - use your histogram making skills from Week 2). Add the mean, meadian, ect from above to your plot.\n",
    "- Who are the 10 artists with happiest and saddest pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705fdd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rank': 3, 'happiness_score': 8.3, 'std_dev': 0.99}\n"
     ]
    }
   ],
   "source": [
    "# read the LabMT word list from the file \"Hedonometer.csv\"\n",
    "import csv\n",
    "def read_labmt_word_list(filename=\"/Users/iben/Desktop/Social graphs/SocialGraphs/Hedonometer.csv\"):\n",
    "    labmt_words = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        # Use DictReader to make the code more readable and less prone to column index errors\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            word = row[\"Word\"]\n",
    "            # Store all information in a nested dictionary\n",
    "            labmt_words[word] = {\n",
    "                \"rank\": int(row[\"Rank\"]),\n",
    "                \"happiness_score\": float(row[\"Happiness Score\"]),\n",
    "                \"std_dev\": float(row[\"Standard Deviation of Ratings\"])\n",
    "            }\n",
    "    return labmt_words\n",
    "\n",
    "LabMT_word_list = read_labmt_word_list()\n",
    "\n",
    "print(LabMT_word_list[\"happy\"])  # Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c1d6d1",
   "metadata": {},
   "source": [
    "#### punkt 1\n",
    "\n",
    "\"For the evaluations, we asked users on Mechanical Turk to rate how a given word made them feel on a nine point integer scale, obtaining 50 independent evaluations per word. We broke the overall assignment into 100 smaller tasks of rating approximately 100 randomly assigned words at a time. We emphasized the scores 1, 3, 5, 7, and 9 by stylized faces, representing a sad to happy spectrum. Such five point scales are in widespread use on the web today (e.g., Amazon) and would likely be familiar with users. The four intermediate scores of 2, 4, 6, 8 allowed for fine tuning of assessments. In using this scheme, we remained consistent with the 1999 Affective Norms for English Words (ANEW) study by Bradley and Lang [55], the results of which we used in constructing our initial metric\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bcda6",
   "metadata": {},
   "source": [
    "#### punkt 2\n",
    "\n",
    "Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0462daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: 5.279999999999999, Valid word count: 5\n",
      "Sentiment score: 6.525, Valid word count: 4\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentiment(tokens, labmt_words):\n",
    "    total_score = 0\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in labmt_words:\n",
    "            total_score += labmt_words[token][\"happiness_score\"]\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0  # Avoid division by zero; return neutral sentiment\n",
    "    return total_score / count, count\n",
    "\n",
    "\n",
    "\n",
    "test_tokens = [\"happy\", \"sad\", \"joyful\", \"angry\", \"ggg\", \"light\", \"better\"]\n",
    "sentiment_score, valid_word_count = calculate_sentiment(test_tokens, LabMT_word_list)\n",
    "print(f\"Sentiment score: {sentiment_score}, Valid word count: {valid_word_count}\")\n",
    "\n",
    "test2_tokens = [\"happy\", \"wonderful\", \"sad\", \"amazing\"]\n",
    "sentiment_score2, valid_word_count2 = calculate_sentiment(test2_tokens, LabMT_word_list)\n",
    "print(f\"Sentiment score: {sentiment_score2}, Valid word count: {valid_word_count2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea68b25",
   "metadata": {},
   "source": [
    "#### punkt 3 \n",
    "Iterage over the nodes in your network, tokenize each page, and calculate sentiment every single page. Now you have sentiment as a new nodal property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
